Below is a **public-GitHub-optimized README**, plus:

1. a **Quickstart demo script** (copy-paste runnable)
2. a **clear architecture diagram** (Markdown / ASCII, GitHub-friendly)

This version is written to:

* attract recruiters
* reassure users itâ€™s real
* keep scope tight and credible

---

# ML Guard

**Lightweight ML Monitoring & Cost Guard for Small Teams**

ML Guard is a minimal, production-grade MLOps monitoring service that tracks **model health, data drift, inference performance, and AWS costs** with almost zero setup.

It is built for teams that want **operational visibility** without running a full MLOps platform.

> Think: *Datadog-lite for machine learning systems.*

---

## âœ¨ Why This Exists

Most ML teams:

* donâ€™t need Kubeflow or full-stack MLOps platforms
* donâ€™t want to maintain Prometheus + Grafana
* **do** want to know when models drift or costs spike

ML Guard focuses on:

* **Actionable signals**
* **Low operational overhead**
* **Fast integration**
* **Clear cost ownership per model**

---

## ðŸš€ Core Features

### Model & Data Monitoring

* Input feature statistics (mean, std, distributions)
* Data drift detection (Population Stability Index)
* Prediction distribution monitoring
* Inference latency (p50 / p95)

### Cloud Cost Monitoring

* AWS Cost Explorer integration
* Cost attribution by **project / model / endpoint**
* Daily cost aggregation
* Cost spike detection

### Alerting

* Slack alerts
* Email alerts
* Threshold-based rules for drift, latency, and cost

---

## ðŸ—ï¸ Architecture

```text
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   ML Application   â”‚
                        â”‚ (batch or REST)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â”‚ inference events
                                  â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   ML Guard SDK     â”‚
                        â”‚ (thin Python lib) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS Cost     â”‚      â”‚   FastAPI Backend  â”‚      â”‚  Dashboard   â”‚
â”‚ Explorer     â”‚â”€â”€â”€â”€â”€â–¶â”‚  - ingest events   â”‚â—€â”€â”€â”€â”€â–¶â”‚ (Next/Dash)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  - API & auth      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ async jobs
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Background Worker  â”‚
                       â”‚ - metrics          â”‚
                       â”‚ - drift            â”‚
                       â”‚ - cost ingestion   â”‚
                       â”‚ - alerts           â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   PostgreSQL       â”‚
                       â”‚ events, metrics,   â”‚
                       â”‚ drift, costs       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” How It Works

1. Your model sends inference events to ML Guard
2. Events are stored in PostgreSQL
3. Background jobs compute:

   * latency & prediction stats
   * feature distributions
   * drift scores
   * AWS cost attribution
4. Alerts fire when thresholds are exceeded
5. A minimal dashboard shows trends over time

---

## ðŸ“¦ Event Ingestion (Core Contract)

Each inference emits a small JSON payload:

```json
{
  "project_id": "proj_123",
  "model_id": "churn_model_v1",
  "endpoint": "predict",
  "timestamp": "2025-01-01T00:00:00Z",
  "latency_ms": 42,
  "y_pred": 1,
  "y_proba": 0.81,
  "features": {
    "age": 29,
    "balance": 1200.5,
    "country": "CA"
  }
}
```

From this single stream, ML Guard derives:

* inference latency metrics
* prediction distributions
* feature statistics
* drift scores over time

---

## âš¡ Quickstart (Local Demo)

### Prerequisites

* Docker
* Docker Compose
* Python 3.10+

---

### 1ï¸âƒ£ Start ML Guard locally

```bash
git clone https://github.com/your-username/ml-guard.git
cd ml-guard

cp .env.example .env
docker compose up --build
```

API will be available at:

```
http://localhost:8000
```

---

### 2ï¸âƒ£ Send demo inference events

Create `demo/send_events.py`:

```python
import time
import random
import requests
from datetime import datetime

API_URL = "http://localhost:8000/api/v1/events"
API_KEY = "demo-key"

headers = {
    "X-API-Key": API_KEY,
    "Content-Type": "application/json"
}

for i in range(500):
    payload = {
        "project_id": "demo_project",
        "model_id": "demo_model_v1",
        "endpoint": "predict",
        "timestamp": datetime.utcnow().isoformat(),
        "latency_ms": random.randint(20, 120),
        "y_pred": random.choice([0, 1]),
        "y_proba": random.random(),
        "features": {
            "age": random.randint(18, 70),
            "balance": random.uniform(0, 5000),
            "country": random.choice(["CA", "US", "UK"])
        }
    }

    requests.post(API_URL, json=payload, headers=headers)
    time.sleep(0.05)

print("âœ… Sent demo events")
```

Run it:

```bash
python demo/send_events.py
```

---

### 3ï¸âƒ£ What youâ€™ll see

* Events stored in PostgreSQL
* Aggregated latency & prediction stats
* Feature distributions per model
* Drift metrics after baseline capture
* (Optional) Slack alerts if thresholds are set

---

## ðŸ“Š Drift Detection

* Numeric features: PSI with configurable bins
* Categorical features: frequency divergence
* Baselines stored per model
* Drift tracked daily and compared to thresholds

---

## ðŸ’° Cost Attribution (AWS)

AWS resources must be tagged with:

* `mlguard:project`
* `mlguard:model`
* `mlguard:endpoint`

ML Guard pulls AWS Cost Explorer data daily and attributes spend accordingly.

This enables:

* per-model cost tracking
* cost spike alerts
* operational cost accountability

---

## ðŸŽ¯ Target Users

* Startups with early production models
* Indie SaaS builders
* Consulting teams
* Small ML teams without dedicated MLOps engineers

---

## ðŸ§ª MVP Scope (Intentional)

* One model type (classification)
* Batch or REST inference
* Custom metrics (no Prometheus)
* AWS-only (v1)
* Minimal dashboard

---

## ðŸ§  Resume Signal

> **Built and deployed a production MLOps monitoring system with drift detection, cost attribution, alerting, and infrastructure-as-code on AWS.**

This project demonstrates:

* MLOps fundamentals
* Cloud cost awareness
* Monitoring & reliability thinking
* Production system design

---

## License

This project is source-available but not open source.

Viewing and evaluation are permitted.
Commercial use, deployment, modification, or redistribution
require explicit permission from the author.

See the LICENSE file for details.

---

```
ml-guard/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api.md
â”‚   â””â”€â”€ runbook.md
â”œâ”€â”€ infra/                        # Terraform: VPC + ECS + RDS + SES/SNS + IAM
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ versions.tf
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ network/
â”‚       â”œâ”€â”€ ecs_service/
â”‚       â”œâ”€â”€ rds_postgres/
â”‚       â””â”€â”€ iam_cost_explorer/
â”œâ”€â”€ backend/                      # FastAPI
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ uv.lock (or requirements.txt)
â”‚   â”œâ”€â”€ alembic.ini
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â”‚   â””â”€â”€ security.py
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”‚   â”œâ”€â”€ projects.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”‚   â”œâ”€â”€ alerts.py
â”‚   â”‚   â”‚   â””â”€â”€ billing.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ drift.py
â”‚   â”‚   â”‚   â”œâ”€â”€ psi.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cost_explorer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ alert_router.py
â”‚   â”‚   â”‚   â””â”€â”€ scheduler.py
â”‚   â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”‚   â””â”€â”€ jobs.py
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚       â”œâ”€â”€ ingest.py
â”‚   â”‚       â”œâ”€â”€ metrics.py
â”‚   â”‚       â””â”€â”€ alerts.py
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/                     # minimal dashboard
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ next.config.js
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ lib/api.ts
â”‚   â””â”€â”€ Dockerfile
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â”œâ”€â”€ backend-ci.yml
        â”œâ”€â”€ frontend-ci.yml
        â””â”€â”€ deploy.yml
```
